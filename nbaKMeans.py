# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/164N82cTXVmN-O1xK_2HbR8I22m8vYrKx

# NBA Player KMeans Distribution
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


from sklearn.cluster import KMeans

import streamlit as st

@st.cache_data
def get_data(file_path):
  data = pd.read_csv(file_path)
  return data

with st.spinner("loading data..."):
  data = get_data('Regular_Season.csv')

tab1, tab2 = st.tabs(['documentation', 'test KMeans'])
with tab1:
  """### **Explore Data**

  Using regular season stats from season 2012-13 to 2022-23, approximately 10 year span of data.

  - Display data
  - Show all columns in dataset
  """
  with st.expander("View Columns in Dataset"):
    data.head()

    data.columns
  st.markdown('---')

  st.subheader("Data cleaning  + Data Engineering")
  st.write("""
           Data Cleaning
           - Cleaning this data frame entailed reformatting the year column to the base year
           - removing 'unnamed' 'season_type' columns, and 
           - mapping the player and team columns to their associated id columns (categorical to numerical) 
           - dropping the player and team columns to complete KMeans analysis

           Data Engineering
           - Calculated total production column
             
           """)
  st.write("")
  with st.expander("View Data Engineering Process"):
    # drop unnamed columns
    data.copy()

    data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)

    # reformat season_type
    data['Season_type'] = data['Season_type'].replace('Regular_Season', 'regular')

    # reformat year column
    data['year'] = data['year'].str.split('-').str[0]

    data.head()

    # map the player and team columns

    # Mapping player name to player ID
    player_map = dict(zip(data['PLAYER_ID'], data['PLAYER']))

    # Mapping team name to team ID
    team_map = dict(zip(data['TEAM_ID'], data['TEAM']))

    # Drop the original name columns
    data.drop(columns=['PLAYER', 'TEAM', 'Season_type'], inplace=True)

    calculate_production_text = """
    ### Total Production Calculation

    **two production** = `two made - two missed * two`   
    **three production** = `three made - three missed * three`     
    **ft production** = `ft made - ft missed`    
    **total production** = `two production + three production + ft production + Ancillary Production`


    ---

    #### Ancillary Production Calculation
    | Statistic | Value |
    | ----------- | ----------- |
    | assist | .15 |
    | steals | .25 |
    | off rebound | .15 |
    | def rebound | .30 |
    | blocks | .20 |
    | turnovers | -.10 |
    | fouls | .05 |
    --- 
    """
    st.markdown(calculate_production_text)
    
    # Calculate two production
    # 1. Calculated missed shots

    data['two_missed'] = data['FGA'] - data['FGM']
    data['three_missed'] = data['FG3A'] - data['FG3M']
    data['ft_missed'] = data['FTA'] - data['FTM']

    # 2. Subtract made shots from missed shots and multiply point value

    data['two_production'] = (data['FGM'] - data['two_missed']) * 2
    data['three_production'] = (data['FG3M'] - data['three_missed']) * 3
    data['ft_production'] = (data['FTM'] - data['ft_missed'])
    data['shooting_production'] = data['two_production'] + data['three_production'] + data['ft_production']
    
    # 3. Calcuate Ancillary Production
    data['assist_production'] = data['AST'] * .22
    data['steal_production'] = data['STL'] * .27
    data['dreb_production'] = data['DREB'] * .17
    data['oreb_production'] = data['OREB'] * .22
    data['block_production'] = data['BLK'] * .17
    data['turnover_production'] = data['TOV'] * -.1
    data['foul_production'] = data['PF'] * .05

    data['ancillary_production'] = data['assist_production'] + data['steal_production'] + data['dreb_production'] + data['oreb_production'] + data['block_production'] + data['turnover_production'] + data['foul_production']

    # 4. Calculate Total Production

    data['total_production'] = data['shooting_production'] + data['ancillary_production']

    options = ["Full Cleaned", "Production Cleaned"]
    st.subheader("View Cleaned/Engineered Dataset")
    selection = st.pills("", options, selection_mode="single")
    st.write("")
    if selection == "Full Cleaned":
      st.dataframe(data.head())
    else:
      st.dataframe(data[['PLAYER_ID', 'TEAM_ID', 'two_production', 'three_production', 'ft_production', 'shooting_production', 'ancillary_production', 'total_production']].head())
  st.markdown('---')

  st.subheader("PCA Explained Variance")
  st.write("Determine which PCA group has more or less variance between the features in the group.")
  with st.expander("View PCA Explained Variance"):
    df = data.copy()
    X = df.drop('total_production', axis=1)
    y = df['total_production']

    # Standardize the data using StandardScaler()

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    st.write("❓ Explained variance does this...")
    # explained variance for each principle component
    pca = PCA(n_components=2)
    pca.fit(X_scaled)
    explained_variance = pca.explained_variance_ratio_


    # 5. Visualizing explained variance
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, align='center')
    ax.set_xlabel('Principal Components')
    ax.set_ylabel('Explained Variance Ratio')
    ax.set_title('Explained Variance Ratio of Each Principal Component')

    # Render in Streamlit
    st.pyplot(fig)

    # Display the components and their feature names
    feature_names = X.columns

    # component matrix
    components = pd.DataFrame(pca.components_, columns=feature_names, index=[f"PC{i+1}" for i in range (len(explained_variance))])
    st.dataframe(components)

  st.markdown("---")

  st.subheader("Elbow Curve")
  st.write("The elbow curve uses the pca clusters to plot `inertia` (how tight the clusters) vs the number of clusters `k` suggest the best number of k clusters that should be used to represent that data using KMeans.")
  with st.expander("Elbow Curve"):
    """#### Elbow Curve"""


    X_pca = pca.fit_transform(X_scaled)

    inertia = []

    k_range = range(1, 11)

    for k in k_range:
      kmeans = KMeans(n_clusters=k, random_state=42)
      kmeans.fit(X_pca)
      inertia.append(kmeans.inertia_)

    # plot the elbow curve
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.plot(k_range, inertia, marker='o', linestyle='-', color='b')
    ax.set_title('Elbow Curve for KMeans Clustering')
    ax.set_xlabel('Number of Clusters (k)')
    ax.set_ylabel('Inertia')
    ax.set_xticks(k_range)
    ax.grid(True)

    # Render in Streamlit
    st.pyplot(fig)

    result_text = """ #### **Elbow Curve Results**

    Based on the length betwee each node, it seems like the momentum slows down drastically at 2 however, there not a drastic significance between each node after approximately 3.

    # **k = 3**

    ---
    """
    st.markdown(result_text)
  

with tab2:
  # add player & team names back to data
  data['PLAYER_ID'] = data['PLAYER_ID'].astype(int)
  data['TEAM_ID'] = data['TEAM_ID'].astype(int)

  data['PLAYER'] = data['PLAYER_ID'].map(player_map)
  data['TEAM'] = data['TEAM_ID'].map(team_map)



  st.subheader("Instructions")
  st.markdown("""
              Default: The **number of clusters** and **team** drop down are selected. 
              You: Change the number of clusters to the 
              
              """)
  col1, col2, col3 = st.columns(3)
  with col1:
    year_options = ["All"] + sorted(data["year"].dropna().unique().tolist())
    year = st.selectbox("Season", year_options, index=0)

  # ---- filter by year to drive to team list ----
  df_year = data.copy() if year == "All" else data.loc[data["year"].astype(str) == str(year)].copy()

  # Team options for that year
  with col2:
    team_options = ["League"] + sorted(df_year["TEAM"].dropna().unique().tolist())
    team = st.selectbox("Choose A Team", team_options, index=0)
  with col3:
      k_option = st.selectbox("Choose The Number of Clusters", list(range(2,10)), index=1)

# --- Final dataset based on team selection ---
if team != "League":
    # Filter to the team, then average per player across duplicate rows (e.g., multiple seasons/games)
    df_team = df_year[df_year['TEAM'] == team].copy()

    # numeric columns to average (exclude ID-like numeric fields)
    num_cols = df_team.select_dtypes(include='number').columns.tolist()
    id_like = {'PLAYER_ID', 'TEAM_ID'}  # don't average these
    agg_num_cols = [c for c in num_cols if c not in id_like]

    # average per player (keep identifiers in groupby keys)
    dataset = (
        df_team
        .groupby(['PLAYER', 'PLAYER_ID', 'TEAM', 'TEAM_ID'], as_index=False)[agg_num_cols]
        .mean()
    )

    # (optional) add how many rows were averaged per player
    counts = (
        df_team.groupby(['PLAYER', 'PLAYER_ID'])['total_production']
        .size().reset_index(name='rows_aggregated')
    )
    dataset = dataset.merge(counts, on=['PLAYER','PLAYER_ID'], how='left')

else:
    # League view unchanged (all rows across teams for that year selection)
    dataset = df_year.copy()


# Select numeric feature columns, excluding IDs and cluster labels
exclude_cols = {'PLAYER', 'TEAM', 'PLAYER_ID', 'TEAM_ID', 'cluster'}
X = dataset[[c for c in dataset.select_dtypes(include='number').columns if c not in exclude_cols]]

# Scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA
n_comp = min(2, X_scaled.shape[0], X_scaled.shape[1])
pca = PCA(n_components=n_comp)
X_pca = pca.fit_transform(X_scaled)

# Fake a second axis if PCA is 1D (for plotting)
X_plot = X_pca if X_pca.shape[1] == 2 else np.c_[X_pca.ravel(), np.zeros(X_pca.shape[0])]

# KMeans
k = min(k_option, len(dataset))
kmeans = KMeans(n_clusters=k, random_state=42, n_init="auto")
dataset['cluster'] = kmeans.fit_predict(X_plot)


# --- Scale ---
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --- PCA (re-run on THIS filtered data) ---
# use up to 2 components; if only 1 is possible, we’ll still plot
n_comp = min(2, X_scaled.shape[0], X_scaled.shape[1])
pca = PCA(n_components=n_comp)
X_pca = pca.fit_transform(X_scaled)

# build 2D array for plotting even if PCA is 1D
X_plot = X_pca if X_pca.shape[1] == 2 else np.c_[X_pca.ravel(), np.zeros(X_pca.shape[0])]

# --- KMeans on the same matrix we plot ---
k = min(k, len(dataset))  # simple guard so k ≤ samples
kmeans = KMeans(n_clusters=k, random_state=42, n_init="auto")
dataset["cluster"] = kmeans.fit_predict(X_plot)


st.subheader(f"KMeans Clustering — {('League' if team=='League' else team)} — {year}")
# --- Plot ---
fig, ax = plt.subplots(figsize=(8, 6))
scatter = ax.scatter(X_plot[:, 0], X_plot[:, 1], c=dataset["cluster"], cmap="viridis", marker="o")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2" if X_pca.shape[1] == 2 else "0")
plt.colorbar(scatter, label="Cluster")
plt.grid(True)
st.pyplot(fig)

with st.expander("View Player Clusters"):
# --- Table ---
  cols_to_show = [c for c in ["PLAYER","TEAM"] if c in dataset.columns] + ["cluster"]
  st.dataframe(dataset[cols_to_show])